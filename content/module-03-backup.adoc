= Red Hat Advanced Cluster Management for Kubernetes

In this session, participants will learn how to work with Red Hat Advanced Cluster Management for Kubernetes (RHACM).

Participants will walk through these 4 major use cases:

- Multi-Cluster Lifecycle
* Centrally manage, create, update, and delete Kubernetes clusters.
* Search, find and modify any kubernetes resource across the entire domain.
* Quickly troubleshoot and resolve issues across your federated domain.

- Virtual Machine Lifecycle
* Centrally manage, create, update, and delete Virtual Machines.
* Search, find and modify any VMs across the entire domain Managed by RHACM.
* Quickly troubleshoot and resolve issues across your VMs with Observability.

- Policy Driven Governance
* Centrally set and enforce policies for security, applications, and infrastructure.
* Quickly visualize detailed auditing on configuration of apps and clusters.
* Leverage Policy Generator to generate policies at scale.

- Advanced Application Lifecycle
* Easily deploy applications at scale leveraging GitOps.
* See applications work real time from multiple sources.
* Quickly visualize application relationships across clusters and those that span clusters.

Lets get started!

[[cluster-lifecycle]]

== Working with Kubernetes Clusters and Cluster Lifecycle

At a high level Cluster Lifecycle management is about creating, upgrading, and destroying and importing clusters in a multi cloud environment, whether they may be traditional OpenShift Clusters, managed environments like ROSA or ARO, or specialized cluster deployments like Hosted Control Planes or Single-Node OpenShift.

NOTE: Please reference the introduction page to find the credentials required to login to your lab environment.

Once you have logged in to OpenShift Console, you will find yourself on the RHACM interface.  

If not, then navigate to the *Cluster* drop down menu and then select *All Clusters*. 

image::101-local-cluster.png[link=self, window=blank, width=100%, Cluster Selection Menu]

In this interface you will see 2 clusters available, the first cluster is a Hosted Control Plane Cluster called *development*.

image::102-cluster-view.png[link=self, window=blank, width=100%, View of Clusters Listed]

[[hosted-control-planes]]

== Working with Hosted Control Planes

Hosted Control Planes is a form factor of Red Hat OpenShift Container platform, but it follows a distinct architectural model.
In standalone OpenShift, the control plane and data plane are coupled in terms of locality. A dedicated group of nodes hosts the control plane with a minimum number to ensure a quorum and the network stack is shared. While functional, this approach may not always meet customers' diverse use cases, especially when it comes to multi-cluster scale deployments.
To address this, Red Hat provides hosted control planes in addition to standalone OpenShift. Hosted Control Planes is based on the upstream Red Hat project HyperShift which can be thought of as a middleware for hosting OpenShift control planes at scale. This deployment model helps solve concerns in regard to cost and time to provision, as well as providing a strong separation between management and workloads.

Hosted Control Planes for Red Hat OpenShift decouple the control plane from the data plane:

- It provides network domain separation between control-plane and workloads.
- Offers a shared interface for fleet administrators and Site Reliability Engineers (SREs) to operate multiple clusters easily.
- Treats the control plane like any other workload, enabling administrators to use the same stack to monitor, secure, and operate their applications while managing the control plane.

The decoupling of the Control Plane and Data Plane introduces multiple potential benefits and paves the way for a Hybrid-cloud approach. Below are possibilities that Hosted Control Plane as a technology enables:

- Trust Segmentation & Human Error Reduction: Management plane for control planes and cloud credentials separate from the end-user cluster. A separate network of management from the workload. Furthermore, with the control-plane managed, it is harder for users to basically shoot themselves in the foot and destroy their own clusters since they won’t be seeing the CP resources in the first place.
- Cheaper Control Planes: You can host \~7-21 control planes into the same three machines you were using for 1 control plane. And run \~1000 control planes on 150 nodes. Thus you run most densely on existing hardware. Which also makes HA clusters cheaper.
- Immediate Clusters: Since the control plane consists of pods being launched on OpenShift, you are not waiting for machines to provision.
- Kubernetes Managing Kubernetes: Having control-plane as Kubernetes workloads immediately unlocks for free all the features of Kubernetes such as HPA/VPA, cheap HA in the form of replicas, control-plane Hibernation now that control-plane is represented as deployments, pods, etc.
- Infra Component Isolation: Registries, HAProxy, Cluster Monitoring, Storage Nodes, and other infra type components are allowed to be pushed out to the tenant’s cloud provider account isolating their usage of those to just themselves.
- Increased Life Cycle Options: You can upgrade the consolidated control planes out of cycle from the segmented worker nodes, including embargoed CVEs.
- Future Mixed Management & Workers IaaS: Although it is not in the solution today, you feel you could get to running the control plane on a different IaaS provider than the workers faster under this architecture
- Heterogeneous Arch Clusters: you can more easily run control planes on one CPU chip type (ie x86) and the workers on a different one (ie ARM or even Power/Z).
- Easier Multi-Cluster Management: More centralized multi-cluster management which results in fewer external factors influencing the cluster status and consistency
- Cross Cluster Delivery Benefits: As you look to have more and more layered offerings such as service mesh, server-less, pipelines, and other span multiple clusters, having a concept of externalized control planes may make delivering such solutions easier.
- Easy Operability: Think about SREs. Instead of chasing down cluster control-planes, they would now have a central-pane of glass where they could debug and navigate their way even to cluster dataplanes. Centralized operations, less Time To Resolution (TTR), and higher productivity become low-hanging fruits.

You will also find a second cluster called *local-cluster*. This cluster is the hub where the Advanced Cluster Management For Kubernetes, and Advanced Cluster Security for Kubernetes resides.

Feel free to navigate the cluster’s interface and explore the different day 2 actions you can perform in the cluster.

[[create-manage-cluster]]

== Create and Manage Clusters

Red Hat Advanced Cluster Management for Kubernetes makes it quite easy to deploy and manage additional clusters. While simplicity is often a winning formula when it comes to deployments of Red Hat OpenShift with methods such as IPI and the Assisted Installer, RHACM takes this to a whole new level with just a few clicks through the cluster creation wizard.

From the Clusters screen you can see how easy it is to deploy a new cluster.

Click on the *Create cluster* button in the center of the screen:

image::103-create-cluster.png[link=self, window=blank, width=100%, Create Cluster]

You will notice that there is an Option for AWS and it’s already highlighted that you have saved credentials. you will use this to deploy our new cluster, but feel free to explore this window and see other cluster types that are available. 

When you are ready, click on the AWS button.

image::104-aws-credentials.png[link=self, window=blank, width=100%, AWS Credentials]

You will see two options for the control plane type: 

.Hosted Control Plane
.Standalone

The *development* cluster that you have provisioned is an example of a Hosted Control Plane cluster, which you explained the benefits of in detail in the link:module-01.html#hosted-control-planes[Introducing Hosted Control Planes] section above. 

For our lab, you will be using the Standalone cluster option. Click on that option, and you will be presented with a menu that allows you to customize the cluster. 

Name your cluster *demo-cluster*, and select *default* for the cluster set. Lastly select the most recent release image *OpenShift 4.17.11*. 

Click on *Next* to continue.

image::105-create-cluster-details.png[link=self, window=blank, width=100%, Create Cluster Details]

On the next screen You can customize the AWS region, the CPU architecture, and the number of nodes to deploy in the control plane and worker pools. 

Click on *Next* to proceed.

image::106-create-cluster-nodepools.png[link=self, window=blank, width=100%, Create Cluster NodePools]

The next screen allows you to configure networking type to use and it's associated  variables. 

Click on *Next* to proceed.

image::107-create-cluster-networking.png[link=self, window=blank, width=100%, Create Cluster Networking]

The next couple of screens allow for additional customization, configuring a proxy, creating private AWS configurations, and pre-configuring automation functions with Ansible Automation Platform. 

Click *Next* on each screen to proceed to the final *Review and Create* screen.

You will see a description of the cluster you are creating, click the blue *Create* button to start the deployment process.

image::108-create-cluster-summary-create.png[link=self, window=blank, width=100%, Create Cluster Summary Page]

If you click on *Clusters* in the left menu bar you will be returned to the original cluster view screen but you can see that our new cluster is now in the creating stage.

image::109-view-new-cluster.png[link=self, window=blank, width=100%, View New Cluster]

NOTE: The deployment of a full cluster will take approximately 45 minutes to complete, the primary purpose of this part of the lab was to demonstrate how easy it is to deploy clusters. you will continue the lab working with the infrastructure already in place.

[[create-manage-vms]]

== Create and Manage Virtual Machines

Do you want to manage and provision your OpenShift Virtualization virtual machine workloads across multiple clusters while using a single source of truth in the GitOps way? In this exercise you will show how you can do that with Red Hat Advanced Cluster Management and OpenShift GitOps.

For this process you leverage OpenShift Virtualization. OpenShift Virtualization leverages Kubevirt an open source project that makes it possible to run, deploy, and manage virtual machines (VMs) with Kubernetes as the underlying orchestration platform. The process of running virtual machines within containers is known as container-native virtualization. OpenShift Virtualization enables container-native virtualization by packaging those virtual machines inside containers and managing both workloads from a single RHACM Console.

=== How does OpenShift Virtualization work?

Open Shift Virtualization leverages *KubeVirt* and it works by extending the Kubernetes application programming interface (API) so it can interact with virtual machines in the same way as other Kubernetes resources and tools. This makes it possible for containers and virtual machines to share the same cluster, nodes, and networks.

=== OpenShift Virtualization added functionality is composed of 3 main components:

- *Custom resource definitions (CRDs):* A custom resource is an object that extends the Kubernetes API or allows you to introduce your own API into a project or a cluster. A custom resource definition file defines your object kind and lets the API Server handle the entire lifecycle. KubeVirt brings a CRD to the Kubernetes API that enables it to handle virtual machines like other Kubernetes objects (such as pods).

- *Controllers:* A controller is a set of deployments running on the cluster that provide an API endpoint for managing the new KubeVirt CRDs.

- *Agents:* Agents run on a cluster’s worker nodes to manage node tasks related to virtualization.

Another way to think of OpenShift Virtualization is as a pod running with a KVM-based virtual machine inside of it. In Kubernetes, a pod is a group of containers that run together and share the same resources, and KVM (Kernel-based Virtual Machine) is an open source technology that extends the Linux® kernel to function as a hypervisor. With KubeVirt, virtual machine instances run just like pods. This allows OpenShift Virtualization to manage VM states like "stopped," "paused," and "running," as well as perform operations like the provisioning, scheduling, and migration of virtual machines. 

=== Deploying a Virtual Machines Using OpenShift GitOps

Red Hat® OpenShift® GitOps is an operator that provides a workflow that integrates git repositories, continuous integration/continuous delivery (CI/CD) tools, and Kubernetes to realize faster, more secure, scalable software development, without compromising quality.

OpenShift GitOps enables customers to build and integrate declarative git driven CD workflows directly into their application development platform. There’s no single tool that converts a development pipeline to "DevOps". By implementing a GitOps framework, updates and changes are pushed through declarative code, automating infrastructure and deployment requirements, and CI/CD.

OpenShift® GitOps takes advantage of Argo CD and integrates it into Red Hat Advanced Cluster Management for Kubernetes (RHACM) to deliver a consistent, fully supported, declarative Kubernetes platform to configure and use with GitOps principles.

By utilizing RHACM users can now enable the optional Argo CD pull model architecture which offers flexibility that may be better suited for certain scenarios. One of the main use cases for the optional pull model is to address network scenarios where the centralized cluster is unable to reach out to remote clusters, while the remote clusters can communicate with the centralized cluster. In such scenarios, the push model would not be easily feasible.

Argo CD currently utilizes a push model architecture where the workload is pushed from a centralized cluster to remote clusters, requiring a connection from the centralized cluster to the remote destinations.

In the pull model, the Argo CD Application CR is distributed from the centralized cluster to the remote clusters. Each remote cluster independently reconciles and deploys the application using the received CR. Subsequently, the application status is reported back from the remote clusters to the centralized cluster, resulting in a user experience (UX) that is similar to the push model.

Another advantage of the pull model is decentralized control, where each cluster has its own copy of the configuration and is responsible for pulling updates independently. The hub-managed architecture using Argo CD and the pull model can reduce the need for a centralized system to manage the configurations of all target clusters, making the system more scalable and easier to manage. However, note that the hub cluster itself still represents a potential single point of failure, which you should address through redundancy or other means.

Additionally, the pull model provides more flexibility, allowing clusters to pull updates on their own schedule and reducing the risk of conflicts or disruptions.

For this exercise you will use the Push Model.

NOTE: ArgoCD has been deployed in your enviroment however you will need to configure it in RHACM.

=== Integrating ArgoCD with RHACM

- Navigate to *Applications* from the left side menu.
- Click *Create application, select ArgoCD AppicationSet-Push Model*.
- Under the Argo server select *Add Argo Server* 

- Enter the following information:
* *Name:* openshift-gitops
* *Namespace:* openshift-gitops
* *ClusterSet:* default

image::03-argoconfig.png[link=self, window=blank, width=100%, ArgoCD Config]

=== Deploying an Virtual Machine 

- Navigate to *Applications* from the left side menu.
- Click *Create application, select ArgoCD AppicationSet-Push Model*.

- Enter the following information:
* *Name:* dev-vm
* *Namespace:* openshift-gitops
* Click *NEXT*

image::03-vm-app-acm.png[link=self, window=blank, width=100%, VM Config]

* Under repository types, select the GIT repository
* *URL:* https://github.com/jalvarez-rh/kubevirt-gitops
* *Revision:* main
* *Path:* vms
* *Destination:* openshift-cnv
* Click *NEXT TWICE*

image::03-vm-app-git.png[link=self, window=blank, width=100%, VM ACM Config]

- Under *Placement* for application deployment, verify that *New Placement* is selected.
* *Cluster set:* default
- Under *Label expressions* click *add label* and select the following
* *Label:* name
* *Operator:* equals any of
* *Values:* local-cluster

image::03-vm-placement-acm.png[link=self, window=blank, width=100%, VM ACM Config 2]

* Click *NEXT - verify that all the information is correct.*
* Click *Submit* 

It will take a few minutes to deploy the application, click on the *Topology Tab* to view and verify that *all of the circles are green*.

image::03-acm-vm-topology.png[link=self, window=blank, width=100%, Application Topology]

Under the Infrastructure view, Select the *Virtual Machines* here you will see a list of the available virtual machines, if you completed all of the steps above you should see a VM labeled *rhel9-gitops* 

image::03-vm-acm-view.png[link=self, window=blank, width=100%, VM View]

From this point you can interact with the virtual machine directly from ACM. Feel free to click the *Launch* button to see all of the information about the Virtual Machine.

image::03-vm-actions.png[link=self, window=blank, width=100%, VM View]

Feel free to experiment with the virtual machine at your leisure, notice all of the different *day 2 operations* available. 

NOTE: Due to time and resource constrains you did not explore the ability to monitor VMs from RHACM leveraging Grafana. If you would like to learn more about the out of the box dashboard that allow you to monitor your VM please visit the following blog to learn more - https://developers.redhat.com/articles/2024/12/05/monitor-openshift-virtualization-scale-red-hat-advanced-cluster-management

Congratulations, you have successfully deployed a Virtual Machine Leveraing Red Hat GitOps. This approach leveraged a Git repository which housed all of the manifests that defined your VMs. RHACM was able to take those manifests and use them as deployables, which were then deployed to the target cluster for easy management of your resources.

[[policy-driven-governance]]

== Policy Driven Governance

Now that you have a cluster and a deployed virtual machines, you need to make sure that infrastructure does not drift from their original configurations. This kind of drift is a serious problem, because it can happen from benign and benevolent fixes and changes, as well as malicious activities that you might not notice but can cause significant problems. The solution that RHACM provides for this is the Governance functionality.

=== Review GRC Functionality

Enterprises must meet internal standards for software engineering, secure engineering, resiliency, security, and regulatory compliance for workloads hosted on private, multi and hybrid clouds. Red Hat Advanced Cluster Management for Kubernetes governance provides an extensible policy framework for enterprises to introduce their own security policies.

The governance lifecycle is based on defined policies, processes, and procedures to manage security and compliance from a central interface page. 

View the following diagram of the governance architecture:

image::113-grc-diagram.png[link=self, window=blank, width=100%, Governance, Risk, Compliance Diagram]

Use the Red Hat Advanced Cluster Management for Kubernetes security policy framework to create and manage policies. Kubernetes custom resource definition instances are used to create policies.

Each Red Hat Advanced Cluster Management policy can have at least one or more templates. For more details about the policy elements, view the Policy YAML table section.

[[create-grc-policies]]

== Creating Policies in RHACM

In order to assist in the creation and management of Red Hat Advanced Cluster Management for Kubernetes policies you use the policy generator tool. This tool, along with GitOps, greatly simplifies the distribution of Kubernetes resource objects to managed OpenShift or Kubernetes clusters through RHACM policies.

=== Prerequisites

To deploy policies with subscriptions, you will need to bind the *open-cluster-management:subscription-admin* ClusterRole to the user creating the subscription.

To do this, complete the following steps:

- Navigate to the *Governance* tab.
- On the top tabs, click on *Policies*.
- Click *Create Policy*.
- On the top switch the toggle to *Display the YAML*.

image::114-policy-toggle.png[link=self, window=blank, width=100%, Display the YAML]

- Copy the following YAML excerpt and paste it in the screen:

[source,yaml,role=execute]
----
apiVersion: policy.open-cluster-management.io/v1
kind: Policy
metadata:
  name: policy-configure-subscription-admin-hub
  namespace: ""
  annotations:
    policy.open-cluster-management.io/standards: NIST SP 800-53
    policy.open-cluster-management.io/categories: CM Configuration Management
    policy.open-cluster-management.io/controls: CM-2 Baseline Configuration
spec:
  remediationAction: inform
  disabled: false
  policy-templates:
    - objectDefinition:
        apiVersion: policy.open-cluster-management.io/v1
        kind: ConfigurationPolicy
        metadata:
          name: policy-configure-subscription-admin-hub
        spec:
          remediationAction: inform
          severity: low
          object-templates:
            - complianceType: musthave
              objectDefinition:
                apiVersion: rbac.authorization.k8s.io/v1
                kind: ClusterRole
                metadata:
                  name: open-cluster-management:subscription-admin
                rules:
                  - apiGroups:
                      - app.k8s.io
                    resources:
                      - applications
                    verbs:
                      - "*"
                  - apiGroups:
                      - apps.open-cluster-management.io
                    resources:
                      - "*"
                    verbs:
                      - "*"
                  - apiGroups:
                      - ""
                    resources:
                      - configmaps
                      - secrets
                      - namespaces
                    verbs:
                      - "*"
            - complianceType: musthave
              objectDefinition:
                apiVersion: rbac.authorization.k8s.io/v1
                kind: ClusterRoleBinding
                metadata:
                  name: open-cluster-management:subscription-admin
                roleRef:
                  name: open-cluster-management:subscription-admin
                  apiGroup: rbac.authorization.k8s.io
                  kind: ClusterRole
                subjects:
                  - name: kube:admin
                    apiGroup: rbac.authorization.k8s.io
                    kind: User
                  - name: system:admin
                    apiGroup: rbac.authorization.k8s.io
                    kind: User
---
apiVersion: policy.open-cluster-management.io/v1
kind: PlacementBinding
metadata:
  name: policy-configure-subscription-admin-hub-placement
  namespace: ""
placementRef:
  name: policy-configure-subscription-admin-hub-placement
  kind: PlacementRule
  apiGroup: apps.open-cluster-management.io
subjects:
  - name: policy-configure-subscription-admin-hub
    kind: Policy
    apiGroup: policy.open-cluster-management.io
---
apiVersion: apps.open-cluster-management.io/v1
kind: PlacementRule
metadata:
  name: policy-configure-subscription-admin-hub-placement
  namespace: ""
spec:
  clusterConditions:
    - status: "True"
      type: ManagedClusterConditionAvailable
  clusterSelector:
    matchExpressions:
      - key: name
        operator: In
        values:
          - local-cluster
----

- Enter a namespace to place the policy, the *default* namespace is OK to use.
- Click *Next* till the end and then click on *Submit*.
- Allow a few moments for the policy to propagate to the *local-cluster / RHACM Hub Cluster*.
- Navigate back to policies and select the *policy-configure-subscription-admin-hub* policy.
- Under the actions dropdown, select *Enforce*. This will enforce the policy, wait until the green checkmark is displayed.

image::115-enforce-governance-policy.png[link=self, window=blank, width=100%, Enforce the Governance Policy]

=== Using Policy Generator

This Policy Generator description will create 2 configuration policies:

- *openshift-gitops-installed*: The goal of the first policy is to inform if the OpenShift GitOps operator is installed on managed clusters.
- *kubeadmin-removed*: The goal of the second policy is to inform if the kubeadmin user is removed from managed clusters.

NOTE: Both policies are informative only, and you will only execute them manually to demonstrate how to resolve issues.

With the ArgoCD resource, you can use Red Hat OpenShift GitOps to manage policy definitions by granting OpenShift GitOps access to create policies on the Red Hat Advanced Cluster Management hub cluster.

==== Prerequisites

We need to create a *ClusterRole* resource for OpenShift GitOps, with access to create, read, update, and delete policies and placements: 

- Create a *ClusterRole* from the OCP Console by Clicking the + Icon on the Top right.

image::04-ocp-console-add.png[link=self, window=blank, width=100%, OCP Console Add]

- Copy and Paste the following YAML, Click *Create*

[source,sh,subs="attributes",role=execute]
----
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: openshift-gitops-policy-admin
rules:
  - verbs:
      - get
      - list
      - watch
      - create
      - update
      - patch
      - delete
    apiGroups:
      - policy.open-cluster-management.io
    resources:
      - policies
      - configurationpolicies
      - certificatepolicies
      - operatorpolicies
      - policysets
      - placementbindings
  - verbs:
      - get
      - list
      - watch
      - create
      - update
      - patch
      - delete
    apiGroups:
      - apps.open-cluster-management.io
    resources:
      - placementrules
  - verbs:
      - get
      - list
      - watch
      - create
      - update
      - patch
      - delete
    apiGroups:
      - cluster.open-cluster-management.io
    resources:
      - placements
      - placements/status
      - placementdecisions
      - placementdecisions/status
----

- Navigate back to the + sign again, you need the *ClusterRoleBinding* object to grant the OpenShift GitOps service account access to the *openshift-gitops-policy-admin* ClusterRole object. Click *Create* once complete

[source,sh,subs="attributes",role=execute]
----
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: openshift-gitops-policy-admin
subjects:
  - kind: ServiceAccount
    name: openshift-gitops-argocd-application-controller
    namespace: openshift-gitops
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: openshift-gitops-policy-admin
----

- Navigate to the Terminal in the Showroom tab and execute the following command: 

[source,sh,subs="attributes",role=execute]
----
oc -n openshift-gitops patch argocd openshift-gitops --type merge --patch "$(curl https://raw.githubusercontent.com/jalvarez-rh/grc-policy-generator-blog/refs/heads/main/openshift-gitops/argocd-patch.yaml)"
----

- This command will merge and patch the ArgoCD configuration to point to the latest Init Container that will allow us to deploy the Policies 

=== Deploying a Policy with ArgoCD

- Navigate to *Applications*.
- Click *Create application, select ArgoCD ApplicationSet - Push Model*.
* Name: policy-generator
* Argo server: openshift-gitops
* Click *NEXT*
* Under repository types, select the GIT repository
* URL: https://github.com/jalvarez-rh/demo-policygenerator.git
* Revision: main
* Path: dir
* Remote namespace: policy-generator
* Click *NEXT* Twice

- Verify that it installs only to the local cluster by setting the following values:

* Cluster sets: *default*
* Label: *local-cluster*
* Operator: *equals any of*
* Value: *true*
- Verify all the information is correct, click *Create*.

It will take a few minutes to deploy the application, *Click on the Topology Tab* to view and verify that *all of the circles are green*.

image::116-governance-topology.png[link=self, window=blank, width=100%, Governance Topology]

- Navigate to the *Governance* tab.
- Click on the *Policies* tab.
- Verify that you see two policies and that their *Cluster Violations* count is one.
* openshift-gitops-installed
* kubeadmin-removed

image::117-policies-list.png[link=self, window=blank, width=100%, Governance Policies List]

Now that the policies have been created for us leveraging the Policy Generator Engine let’s go ahead and enforce them:

- On the *openshift-gitops-installed* policy, click on the ellipses and set policy to *Enforce*.

image::118-policies-enforce-red.png[link=self, window=blank, width=100%, Enforce the Policy]

- Click the *Enforce* button to verify.
- Wait a few minutes and you will see that the *Cluster Violations* will go from *red* to *green*.

image::119-policies-enforce-green.png[link=self, window=blank, width=100%, Policy Enforced]

- Click on the policy and slect *Results* to veify that the gitops operator has been installed.

CAUTION: CAREFUL implementing the *kubeadmin-removed* policy, if you enforce this you won’t be able to continue this lab and access that cluster through the console as the only account created on these clusters is Kubeadmin.

Now you have successfully created a Policy leveraging the Policy Generator to scan your clusters, if you would like to play with other policies please visit the Policy Repo for more Policies you can test out.

[[deploying-applications]]

== Deploying Applications to Managed Clusters in RHACM

Building on the concepts that you just learned on how to deploy Applications with Red Hat GitOps lets now deploy the application that you loaded to your local Quay Repo earlier in the earlier module.

- Navigate to *Applications* from the left side menu.
- Click *Create application*, select *ArgoCD AppicationSet-Push Model*.
- Enter the following information:
* Name: skupper-patient-demo
* Argo Server: openshift-gitops
* Click *NEXT*

image::03-app-gitops.png[link=self, window=blank, width=100%, App GitOps]

* Under repository types, select the GIT repository
* URL: https://github.com/mfosterrox/skupper-security-demo.git
* Revision: main
* Path: skupper-demo
* Destination: patient-portal
* Click *NEXT TWICE*

image::03-app-gitops-2.png[link=self, window=blank, width=100%, App ACM GitOps]

- Under *Sync Policy* uncheck *Automaticaly sync when cluster state changes* and check *Replace resources instead of applying changes from the source repository* 

image::03-app-gitops-3.png[link=self, window=blank, width=100%, App ACM GitOps]

NOTE: These changes are only required as you will be modifying the application YAML on ACM and you don't want it to sync to a Git Repo, you normaly wouldn't uncheck these in a real production enviroment.

- Under *Placement* for application deployment, verify that *New Placement* is selected.
* Cluster set: default
- Under *Label expressions* click *add label* and select the following
* *Label:* name
* *Operator:* equals any off
* *Values:* local-cluster

image::03-app-placement.png[link=self, window=blank, width=100%, ACM App Placement]

- Verify all of the information is correct and click *Submit*.

It will take a few minutes to deploy the application, click on the *Topology Tab* to view and verify that *all of the circles are green*.

image::03-application-topology-git.png[link=self, window=blank, width=100%, Application Topology]

Under the topology view, Select the *Route* and click on the *Launch Route URL*, this will take you to the Front end for the Patient Portal application, which is now running in our Hosted Controlled Plane (HCP) CLuster.

NOTE: If you get a "Application is not available" page change the URL to use http:// and that should fix the issue. 

image::03-application-route-git.png[link=self, window=blank, width=100%, Application Route]

Congratulations! You have successfully deployed an application to using RHACM and OpenShift GitOps. This approach leveraged a Git repository which housed all of the manifests that defined your application. RHACM was able to take those manifests and use them as deployables, which were then deployed to the target cluster.


[[updating-an-application]]

=== Updating the Frontend Application

Building on the concepts that you learned earlier on how to deploy Policies and Applications with Red Hat GitOps let's use your local Quay repo that contains a *frontend* image you built earlier, this image contains a few key security issues that you will explore on the ACS module later on.

- Navigate to *Applications* from the left side menu.
- Click on *Filter* and under *type* select *Application Set* 
- Click on the *patient-demo* App and Click *Topology*
- Click on *deployment* then find *frontend* Click on *frontend*

image::03-deployment-patient.png[link=self, window=blank, width=100%, Application Deployment]

- Click *Launch resource in Search* a new window will pop up

image::03-frontend-search.png[link=self, window=blank, width=100%, Application Frontend]

- Under Deployment find *frontend* and click it

image::03-frontend-search2.png[link=self, window=blank, width=100%, Application Frontend Search]

- Click on the *YAML* tab and under spec:containers find the *image* field

image::03-frontend-search3.png[link=self, window=blank, width=100%, Application Frontend Search Image]

- Navigate back the Terminal screen and execute the following command

[source,sh,subs="attributes",role=execute]
----
echo $QUAY_URL/$QUAY_USER/frontend:0.1
----

- The output from the terminal will provide an URL you need for the frontend image, copy it from the terminal. Below is an example of an expected output

[.console-output]
[source,bash,subs="+macros,+attributes"]
----
[demo-user@bastion ~]$ echo $QUAY_URL/$QUAY_USER/frontend:0.1
quay-pjhhr.apps.cluster-pjhhr.pjhhr.sandbox4183.opentlc.com/quayadmin/frontend:0.1
----

- Navigate back to Search and replace the *image* with the URL you just copy from the terminal. Click *Save*

image::03-frontend-image.png[link=self, window=blank, width=100%, Application Frontend Image New]

- Navigate back to ACM / Application / Topology View 
- Click on Deployment / Frontend. Verify that the image url has changed

image::03-frontend-image2.png[link=self, window=blank, width=100%, Application Frontend Image Toplogy]


NOTE: The *Pod* in the Topology View might go RED, this is expected as you are editing this on the ACM side but not the GitOps side so is reporting a difference, you wouldn't do this on a real production environment, this is only for educational purposes only.

Congratulations, you have successfully deployed an application to a Kubernetes cluster using RHACM. This approach leveraged a Git repository which housed all of the manifests that defined your application. RHACM was able to take those manifests and use them as deployables, which were then deployed to the target cluster.

In the Next Module you will dive deeper into the Security aspects of the Frontend image you just deployed, you will scan it and explore vulnabilities and much more.

[[acm-conclusion]]

== Conclusion

In summary, you made use of the features provided by Red Hat Advanced Cluster Management for Kubernetes, to deploy a brand new standalone cluster, as well as deploy a Virtual Machine using infrastructure as code with RedHat GitOps, you also deployed governance policies and a complex application with GitOps, making it much easier to build, manage and secure your Kubernetes Clusters. Hopefully this lab has helped demonstrate to you the immense value provided by RHACM and OpenShift Platform Plus. Please feel free to continue and explore the RHACM lab environment, or continue on to the next portion of the lab. 